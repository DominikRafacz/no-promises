
input:
numpy's arrays
X_train, X_test : n x p
y_train, y_test : n x q



architecture = Architecture()\
    .addLayer(InputLayer(p))\
    .addLayer(HiddenLayer(p, activation=Sigmoid()))\
    .addLayer(HiddenLayer(p/2, activation=ReLu(), bias=False))\
    .addLayer(HiddenLayer(q, activation=Softmax()))\
    .setInitializationMethod(RandomInitialization())\
    .setLossFunction(LogLoss())

model_1 = architecture.initialize()
model_1.train(X_train, y_train, momentum=0.8, batch_size=100)

model_2 = architecture.initialize()
model_2.train(other_training_plan, X_train, y_train)

model_1.predict(X_test, y_test)


-> Architecture

-> InputLayer
    shape : Int

-> HiddenLayer:
    shape : Int
    weights : Array

    feed_forward(input) : Array
    backpropagate(sth, parameters)

-> ActivationFunction:
    calculate(...)
    derivative(...)

-> LossFunction:
    calculate(...)
    derivative(...)

-> Model:
    input : InputLayer
    layers : List<HiddenLayer>
    training_history : List<(Double, Double)>
    loss_function : LossFunction

    train(X_train : Array, y_train : Array, a lot of other parameters, evaluation_dataset=(X_test,  y_test)) {

        food = batch(X_train)
        for (layer in layers):
            food = layer.feed_forward(food)

        loss = loss_function.calculate(food, batch(y_train))
        error = loss_function.derivative(food, batch(y_train))
        for (layer in inverse(layers)):
             error = layer.backpropagate(error)

    }

    predict(X_test,  y_test)